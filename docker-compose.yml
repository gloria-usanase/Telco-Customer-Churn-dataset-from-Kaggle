version: '3.8'

services:
  # PostgreSQL Database
  postgres:
    image: postgres:15-alpine
    container_name: pipeline_postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    ports:
      - "5433:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./sql/init.sql:/docker-entrypoint-initdb.d/init.sql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - pipeline_network

  # Pipeline Runner (replaces Airflow)
  pipeline:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: pipeline_runner
    environment:
      # Database Connection
      DB_HOST: postgres
      DB_PORT: 5432
      DB_NAME: airflow
      DB_USER: airflow
      DB_PASSWORD: airflow
      
      # Timezone
      TZ: Africa/Kigali
      
    volumes:
      - ./orchestrator.py:/opt/pipeline/orchestrator.py
      - ./scripts:/opt/pipeline/scripts
      - ./sql:/opt/pipeline/sql
      - ./data:/opt/pipeline/data
      - ./logs:/opt/pipeline/logs
      - ./kaggle.json:/root/.kaggle/kaggle.json:ro
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - pipeline_network
    command: >
      bash -c "
        echo '=================================================='
        echo 'Data Pipeline Starting'
        echo '=================================================='
        echo ''
        echo 'Waiting for database to be ready...'
        sleep 5
        echo ''
        echo 'Running pipeline orchestrator...'
        python3 /opt/pipeline/orchestrator.py
        echo ''
        echo '=================================================='
        echo 'Pipeline execution complete!'
        echo '=================================================='
        echo ''
        echo 'To run again: docker-compose restart pipeline'
        echo 'To view logs: docker-compose logs -f pipeline'
        echo 'To validate: docker-compose exec pipeline python3 /opt/pipeline/orchestrator.py'
        echo ''
        echo 'Database available at: localhost:5432'
        echo 'Connect with: docker-compose exec postgres psql -U airflow -d airflow'
        echo ''
        tail -f /dev/null
      "

volumes:
  postgres_data:
    driver: local

networks:
  pipeline_network:
    driver: bridge
